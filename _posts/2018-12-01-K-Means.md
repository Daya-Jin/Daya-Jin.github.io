---
layout: post
title:  "K-Means"
categories: MachineLearning
tags: KMeans clustering
---

* content
{:toc}

# 算法概述

K-Means算法是一种无监督分类算法，假设有无标签数据集：

$$
X=
\left[
\begin{matrix}
 x^{(1)} \\
x^{(2)} \\
 \vdots \\
 x^{(m)} \\
\end{matrix}
\right]
$$

该算法的任务是将数据集聚类成$$k$$个簇$$C={C_{1},C_{2},...,C_{k}}$$，最小化损失函数为：

$$
E=\sum_{i=1}^{k}\sum_{x\in{C_{i}}}||x-\mu_{i}||^{2}
$$

其中$$\mu_{i}$$为簇$$C_{i}$$的中心点：

$$
\mu_{i}=\frac{1}{|C_{i}|}\sum_{x\in{C{i}}}x
$$

要找到以上问题的最优解需要遍历所有可能的簇划分，K-Mmeans算法使用贪心策略求得一个近似解，具体步骤如下：

1. 在样本中随机选取$$k$$个样本点充当各个簇的中心点$$\{\mu_{1},\mu_{2},...,\mu_{k}\}$$
2. 计算所有样本点与各个簇中心之间的距离$$dist(x^{(i)},\mu_{j})$$，然后把样本点划入最近的簇中$$x^{(i)}\in{\mu_{nearest}}$$
3. 根据簇中已有的样本点，重新计算簇中心
   $$\mu_{i}:=\frac{1}{|C_{i}|}\sum_{x\in{C{i}}}x$$
4. 重复2、3


## 改进

K-means算法得到的聚类结果严重依赖与初始簇中心的选择，如果初始簇中心选择不好，就会陷入局部最优解，如下图：

![](/img/2018-10-28_10-17-53.png)

![](/img/2018-10-28_10-18-12.png)

避免这种情况的简单方法是重复多次运行K-means算法，然后取一个平均结果。

另一种更精妙的方法是K-means++，它改进了K-means算法初始中心点的选取，改进后的选取流程如下：

1. 在数据集中随机选取一个样本点作为第一个簇中心$$C_{1}$$
2. 计算剩余样本点与所有簇中心的最短距离，令为$$D(x^{(i)})=min[dist(x^{(i)},C_{1}),dist(x^{(i)},C_{2}),...,dist(x^{(i)},C_{n})]$$，某样本点被选为下一个簇中心的概率为$$\frac{D(x^{(i)})^{2}}{\sum\limits_{x^{(i)}\notin{C}}D(x^{(i)})^{2}}$$
3. 重复2直到选出$$k$$个簇中心

可以看出K-means++算法的思想很简单明了，初始簇中心之间的距离应该越大越好。
