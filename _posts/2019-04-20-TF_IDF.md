---
layout: post
title:  "TF-IDF"
categories: NLP
tags: NLP feature
---

* content
{:toc}

# 概述

TFIDF全称为**词频-逆文档频率**(Term Frequency-Inverse Document Frequency)，最早用于搜索引擎中的匹配打分，在比赛中也可用来提取特征。

**TF**(Term Frequency)指的是单词在当前文档中出现的频数：

$$
tf(t,d)=n_{t,d}
$$

其中$t$表示某个单词term，$d$表示当前文档document，$n$表示出现次数。不难看出若某个文档特别长，那么每个单词的TF都会变大，为了消除这种影响，将TF归一化，除以文档总词数：

$$
tf(t,d)=\frac{n_{t,d}}{\sum_{t'\in{d}}n_{t',d}}
$$

TF表征了文档中各单词的归一化频率，该值越大，说明单词在文档中的占比越大。

**IDF**(Inverse Document Frequency)指的是包含某个词的文档数所占总文档数的比例倒数：

$$
df(t,D)=d_{t} \\
idf(t,D)=\frac{N_{d}}{df(t,D)} \\
$$

其中$D$表示整个文档集，$df(t,D)$表示某一单词的文档频数(document frequency)，即包含某一单词的文档数量；$N_{d}$表示文档总数。为了避免除零，对分母$+1$，且为了避免$idf$值过大，取对数：

$$
idf(t,D)=\log\frac{N_{D}}{df(t,D)+1}
$$

可以看出DF值表征了单词在所有文档库中的常见程度，如果一个单词要能体现出文档的特征，那么应该同时满足TF值大与DF值小，即在某一文档中经常出现，但是在其他文档中不常出现。将DF值取倒数，那么得到TF-IDF权重的计算方式：

$$
tfidf(t,d,D)=tf(t,d)\times{idf(t,D)}
$$

简单实现[见此](https://github.com/Daya-Jin/ML_for_learner/blob/master/feature_extraction/text.py#L48)。
