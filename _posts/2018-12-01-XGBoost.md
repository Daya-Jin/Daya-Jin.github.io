---
layout: post
title:  "XGBoost"
categories: MachineLearning
tags: GBM
---

* content
{:toc}

# XGBoost

K个集成树模型的输出为（加法模型，每次boost只针对上次的残差）：

$$
\hat{y_{i}}=\phi(X_{i})=\sum\limits_{k=1}^{K}f_{k}(X_i)
$$

带正则项的目标函数为：

$$
L(\phi)=\sum\limits_{i}l(\hat{y}_i,y_{i})+\sum\limits_{k}\Omega(f_{k})
$$

其中

$$
\Omega(f_{k})=\gamma{T}+\frac{1}{2}\lambda||w||^{2}
$$

那么，在第t轮迭代时的目标函数可以写成：

$$
L^{(t)}=\sum\limits_{i=1}^{n}l(y_{i},\hat{y}_{i}^{(t-1)}+f_{t}(X_{i}))+\Omega(f_{t})
$$

根据泰勒公式

$$
f(x+\Delta{x})≈f(x)+f'(x)\Delta{x}+\frac{1}{2}f''(x)\Delta{x}^{2}
$$

设$$g_{i}$$、$$h_{i}$$分别为目标函数$$L^{(t)}$$关于$$\hat{y}^{t-1}$$的一阶偏导与二阶偏导，第t轮的目标函数展开到二阶可写成

$$
L^{(t)}≈\sum\limits_{i=1}^{n}[l(y_{i},\hat{y}_{i}^{(t-1)})+g_{i}f_{t}(X_{i})+\frac{1}{2}h_{i}f_{t}^{2}(X_{i})]+\Omega(f_{t})
$$

省略常数项（上一轮的残差）并展开正则项，可写成

$$
\tilde{L}^{(t)}≈\sum\limits_{i=1}^{n}[g_{i}f_{t}(X_{i})+\frac{1}{2}h_{i}f_{t}^{2}(X_{i})]+\gamma{T}+\frac{1}{2}\lambda\sum\limits_{j=1}^{T}w_{j}^{2}
$$

接下来明确一下单颗树的输出，设

$$
f_{k}(X_{i})=w_{q(X_{i})}
$$

其中$$q(X_{i})$$将样本映射到某一叶子节点，$$w_{j}$$表示某一叶子节点的分数，也相当于叶子节点权重。

再设$$I_{j}$$表示第$$j​$$个叶子节点中所有的样本，目标函数可写成：

$$
\begin{align}
\tilde{L}^{(t)} &≈ \sum\limits_{j=1}^{T}[(\sum\limits_{i\in{I_{j}}}g_{i})w_{j}+\frac{1}{2}(\sum\limits_{i\in{I_{j}}}h_{i})w_{j}^{2}]+\gamma{T}+\frac{1}{2}\lambda\sum\limits_{i\in{I_{j}}}^{T}w_{j}^{2} \\
&= \sum\limits_{j=1}^{T}[(\sum\limits_{i\in{I_{j}}}g_{i})w_{j}+\frac{1}{2}(\sum\limits_{i\in{I_{j}}}h_{i}+\lambda)w_{j}^{2}]+\gamma{T} \\
&= \sum\limits_{j=1}^{T}[G_{j}w_{j}+\frac{1}{2}(H_{j}+\lambda)w_{j}^{2}]+\gamma{T} \\
\end{align}
$$

其中$$G_{j}=\sum\limits_{i\in{I_{j}}}g_{i}$$，$$H_{j}=\sum\limits_{i\in{I_{j}}}h_{i}$$，则令偏导等于零时的最优参数为$$w_{j}^{*}=-\frac{G_{j}}{H_{j}+\lambda}$$，最小化后的损失为$$\tilde{L}_{min}=-\frac{1}{2}\sum\limits_{j=1}^{T}\frac{G_{j}^{2}}{H_{j}+\lambda}+\gamma{T}$$，可以看出XGB的总损失跟叶子节点的梯度还有叶子数有关，可以推出XGB在划分数据集（生成树）时的增益：

$$
Gain=\frac{1}{2}[\frac{G_{L}^{2}}{H_{L}+\lambda}+\frac{G_{R}^{2}}{H_{R}+\lambda}-\frac{(G_{L}+G_{R})^{2}}{H_{L}+H_{R}+\lambda}]-\gamma
$$
