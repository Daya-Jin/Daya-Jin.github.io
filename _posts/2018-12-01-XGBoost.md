---
layout: post
title:  "XGBoost"
categories: MachineLearning
tags: GBM
---

* content
{:toc}

# XGBoost

## 模型

K个集成树模型的输出为（加法模型，每次boost只针对上次的残差）：
$$
\hat{y_{i}}=\phi(X_{i})=\sum\limits_{k=1}^{K}f_{k}(X_i)
$$

带正则项的目标函数为：

$$
L(\phi)=\sum\limits_{i}l(\hat{y}_i,y_{i})+\sum\limits_{k}\Omega(f_{k})
$$

其中

$$
\Omega(f_{k})=\gamma{T}+\frac{1}{2}\lambda||w||^{2}
$$

那么，在第t轮迭代时的目标函数可以写成：

$$
L^{(t)}=\sum\limits_{i=1}^{n}l(y_{i},\hat{y}_{i}^{(t-1)}+f_{t}(X_{i}))+\Omega(f_{t})
$$

根据泰勒公式

$$
f(x+\Delta{x})≈f(x)+f'(x)\Delta{x}+\frac{1}{2}f''(x)\Delta{x}^{2}
$$

设$$g_{i}$$、$$h_{i}$$分别为目标函数$$L^{(t)}$$关于$$\hat{y}^{t-1}$$的一阶偏导与二阶偏导，第t轮的目标函数展开到二阶可写成

$$
L^{(t)}≈\sum\limits_{i=1}^{n}[l(y_{i},\hat{y}_{i}^{(t-1)})+g_{i}f_{t}(X_{i})+\frac{1}{2}h_{i}f_{t}^{2}(X_{i})]+\Omega(f_{t})
$$

省略常数项（上一轮的残差）并展开正则项，可写成

$$
\tilde{L}^{(t)}=\sum\limits_{i=1}^{n}[g_{i}f_{t}(X_{i})+\frac{1}{2}h_{i}f_{t}^{2}(X_{i})]+\gamma{T}+\frac{1}{2}\lambda\sum\limits_{j=1}^{T}w_{j}^{2}
$$

接下来明确一下单颗树的输出，设

$$
f_{k}(X_{i})=w_{q(X_{i})}
$$

其中$$q(X_{i})$$将样本映射到某一叶子节点，$$w_{j}$$表示某一叶子节点的分数。

再设$$I_{j}$$表示第$$j$$个叶子节点中所有的样本，目标函数可写成：

$$
\begin{align}
\tilde{L}^{(t)} &≈ \sum\limits_{j=1}^{T}[(\sum\limits_{i\in{I_{j}}}g_{i})w_{j}+\frac{1}{2}(\sum\limits_{i\in{I_{j}}}h_{i})w_{j}^{2}]+\gamma{T}+\frac{1}{2}\lambda\sum\limits_{i\in{I_{j}}}^{T}w_{j}^{2} \\
&= \sum\limits_{j=1}^{T}[(\sum\limits_{i\in{I_{j}}}g_{i})w_{j}+\frac{1}{2}(\sum\limits_{i\in{I_{j}}}h_{i}+\lambda)w_{j}^{2}]+\gamma{T} \\
&= \sum\limits_{j=1}^{T}[G_{j}w_{j}+\frac{1}{2}(H_{j}+\lambda)w_{j}^{2}]+\gamma{T} \\
\end{align}
$$

其中$$G_{j}=\sum\limits_{i\in{I_{j}}}g_{i}$$，$$H_{j}=\sum\limits_{i\in{I_{j}}}h_{i}$$，则令偏导等于零时的最优参数为$$w_{j}^{*}=-\frac{G_{j}}{H_{j}+\lambda}$$，最小化后的损失为$$\tilde{L}_{min}=-\frac{1}{2}\sum\limits_{j=1}^{T}\frac{G_{j}^{2}}{H_{j}+\lambda}+\gamma{T}$$，可以看出XGB的总损失跟叶子节点的梯度还有叶子数有关，可以推出XGB在划分数据集（生成树）时的增益：

$$
Gain=\frac{1}{2}[\frac{G_{L}^{2}}{H_{L}+\lambda}+\frac{G_{R}^{2}}{H_{R}+\lambda}-\frac{(G_{L}+G_{R})^{2}}{H_{L}+H_{R}+\lambda}]-\gamma
$$

除了损失函数中的正则项，XGB还支持**列采样**(column subsampling)来避免过拟合。



## 加速

**近似分割算法**

在DT算法算法中，关键就在于找到一个最佳分割点，如CART对连续型特征的处理，必须先对特征的所有值进行排序，然后再遍历尝试所有特征中所有可能的切分点，找到一个最佳位置，这种操作计算量很大。XGB采用了一种快速的近似分割算法：根据百分位点选出一些**候选分割点**(proposed candidate points)，从这些候选分割点中选取分割位置。这种近似分割算法有有两种实现方式，一种是**全局**(global)的，在一棵树生成之前就定好候选分割点，然后对这棵树的所有分割均在这些全局分割点上进行；另一种是**局部**(local)的，在每次分割时都重新推举候选分割点。全局实现的步骤数要少于局部实现，但是却需要更多的候选分割点，因为局部实现能在每次分割后**重调**(refine)候选分割点。

值得一提的是，在跟据分位点推举候选分割点时，这个百分点指的并不是统计意义上的百分点，而是根据二阶梯度确定的分位点：
$$
r_k(z)=\frac{1}{\sum_{D_{k}}h}\sum\limits_{D_{k},x<z}h
$$
其中$$D_{k}$$表示第$$k$$个特征集，$$h$$表示二阶特征。那么近似分割算法在第$$k$$个特征上找到的$$l$$个候选分割点需要满足以下条件：
$$
|r_{k}(s_{k,j})-r_{k(s_{k,j+1})}|<{\epsilon}, \quad s_{k,1}={\min}X_{{\cdot}k},s_{k,l}={\max}X_{{\cdot}k}
$$
其中$$s_{k,j}$$表示在第$$k$$个特征上的第$$j$$个候选分割点。(为什么选用二阶导数待补充，论文中公式3的重写推出来不对)



**稀疏感知**

在DT算法中分割遇到缺失值时，样本会复制多份进入所有子树；而在XGB中，为带缺失值的样本或从未出现过的值规定了一个**默认方向**(default direction)，这个特定特征下的默认方向也是从数据中学到的。在模型训练，即树的生成时，假设在对第$$k$$个特征做test，数据中某些样本的第$$k$$个特征是缺失的，那么在确定分割点与默认方向时，首先尝试将所有缺失值样本划到左边计算一次$$gain_{left}$$，再尝试将缺失值样本划到右边计算一次$$gain_{right}$$，增益值最大的方向被设为默认方向。



