---
layout: post
title:  "LightGBM"
categories: MachineLearning
tags: GBM
---

* content
{:toc}

# LightGBM

因为前文已经详细讲过XGB的原理，此文只对照XGB做一下两者的比较。

## Histogram

XGB中使用了一个近似分割算法来寻找某一特征中的分割位置，首先将特征进行排序，再根据二阶梯度搜索一个最佳的百分位分割点。而lightGBM中对这一过程的处理更加粗略，对特征做分箱操作(形成一个直方图)，然后在各箱之间寻找一个最优分割点。很明显后者的处理方式找到的是一个很粗略的分割位置，相比于XGB必然损失了精度，但是从另一个角度来看，此举起到了一定的正则化效果。

在树的分裂过程时，父节点分裂成两个子节点，不难得到两子节点的直方图的和等于父节点的直方图。那么父节点的直方图是已经计算过的，然后只需要再计算两子节点中较小节点的直方图，那么另一个子节点的直方图就可以通过做减法得到。

LightGBM的分箱操作相当于是对数据做了特征离散化，这肯定会减少存储上的开销。

## Leaf-wise tree growth

我们之前在讲树模型的生成时，默认其在实现上的策略是Level-wise的，意思是尽量会去分裂同一层的节点，然后再分裂下一层的节点，知道某一边的节点分裂增益小于阈值或样本数小于阈值。Level-wise的优点在于可并行分裂，但是他不能保证在分裂次数相同的情况下产生最大的增益贡献。比如左分支先分裂产生两个叶子节点，那么此时有三个分裂选择，分裂左分支的两个子节点或者分裂右分支(假设均符合分裂条件)，而Level-wise策略会选择分裂右分支。