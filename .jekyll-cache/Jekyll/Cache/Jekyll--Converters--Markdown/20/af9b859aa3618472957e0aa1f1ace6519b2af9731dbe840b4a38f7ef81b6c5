I"÷<ul id="markdown-toc">
  <li><a href="#æ¨¡å‹æ¦‚è¿°" id="markdown-toc-æ¨¡å‹æ¦‚è¿°">æ¨¡å‹æ¦‚è¿°</a>    <ul>
      <li><a href="#å†³ç­–è¾¹ç•Œ" id="markdown-toc-å†³ç­–è¾¹ç•Œ">å†³ç­–è¾¹ç•Œ</a></li>
    </ul>
  </li>
  <li><a href="#åè®°" id="markdown-toc-åè®°">åè®°</a></li>
</ul>

<h1 id="æ¨¡å‹æ¦‚è¿°">æ¨¡å‹æ¦‚è¿°</h1>

<p>å‡å®šæœ‰ä¸€ç»„æ•°æ®$X$ä¸$Y$ï¼Œå…¶ä¸­</p>

\[X=
\left[
\begin{matrix}
 x^{(1)} \\
x^{(2)} \\
 \vdots \\
 x^{(m)} \\
\end{matrix}
\right]\]

<p>$X$æ€»å…±åŒ…å«$m$æ¡æ•°æ®ï¼Œè€Œæ¯æ¡æ•°æ®$x^{(i)}$åˆå¯è¡¨ç¤ºä¸ºï¼š</p>

\[x^{(i)}=
\left[
\begin{matrix}
 x^{i}_{0} &amp; x^{i}_{1} &amp; \cdots &amp; x^{i}_{n}
\end{matrix}
\right]\]

<p>$Y$æ˜¯ä¸€ç»„å‘é‡ï¼Œå…·ä½“å±•å¼€ä¸ºï¼š</p>

\[Y=
\left[
\begin{matrix}
 y^{(1)} \\
y^{(2)} \\
 \vdots \\
y^{(m)} \\
\end{matrix}
\right]\]

<p>å‡è®¾äºŒåˆ†ç±»æ•°æ®æœä»ä¼¯åŠªåˆ©åˆ†å¸ƒï¼Œç‰¹å¾æ¡ä»¶æ¦‚ç‡æœä»é«˜æ–¯åˆ†å¸ƒï¼š</p>

\[P(y=1)=\phi \\
P(y=0)=1-\phi \\
P(x|y=1)=\frac{1}{\sqrt{2\pi}\sigma}exp[-\frac{(x-\mu_{1})^{2}}{2\sigma^{2}}] \\
P(x|y=0)=\frac{1}{\sqrt{2\pi}\sigma}exp[-\frac{(x-\mu_{0})^{2}}{2\sigma^{2}}] \\\]

<p>ç»™å®šä¸€ä¸ªæ ·æœ¬ï¼Œæ¨¡å‹çš„è¾“å‡ºä¸ºï¼š</p>

\[\begin{aligned}
P(y=1|x)&amp;=\frac{P(y=1)P(x|y=1)}{P(y=0)P(x|y=0)+P(y=1)P(x|y=1)} \\
&amp;=\frac{1}{1+\frac{P(y=0)P(x|y=0)}{P(y=1)P(x|y=1)}} \\
&amp;=\frac{1}{1+\frac{1-\phi}{\phi}exp[-\frac{(x-\mu_{0})^{2}}{2\sigma^{2}}+\frac{(x-\mu_{1})^{2}}{2\sigma^{2}}]} \\
&amp;=\frac{1}{1+\frac{1-\phi}{\phi}exp\frac{2(\mu_{0}-\mu_{1})x+\mu_{1}^{2}-\mu_{0}^{2}}{2\sigma^{2}}} \\
&amp;=\frac{1}{1+exp(\frac{\mu_{0}-\mu_{1}}{\sigma^{2}}x+\frac{\mu_{1}^{2}-\mu_{0}^{2}}{2\sigma^{2}}+\ln(\frac{1-\phi}{\phi}))}
\end{aligned}\]

<p>ä»¤$-a=\frac{\mu_{0}-\mu_{1}}{\sigma^{2}}$ï¼Œ$-b=\frac{\mu_{1}^{2}-\mu_{0}^{2}}{2\sigma^{2}}+\ln(\frac{1-\phi}{\phi})$ï¼Œå¾—ï¼š</p>

\[P(y=1|x)=\frac{1}{1+e^{-(ax+b)}}\]

<p>å¼•å…¥<strong>å‡ ç‡</strong>(odds)æ¦‚å¿µï¼š</p>

\[\begin{aligned}
odds&amp;=\frac{P(y=1|x)}{P(y=0|x)} \\
&amp;=\frac{P(y=1|x)}{1-P(y=1|x)} \\
&amp;=e^{ax+b}
\end{aligned}\]

<p>ä¸¤è¾¹åŒæ—¶å–å¯¹æ•°ï¼š</p>

\[\begin{aligned}
\ln(\frac{P(y=1|x)}{P(y=0|x)})&amp;=ax+b
\end{aligned}\]

<p>ç”±æ­¤å¼•å‡ºLogistic Regressionçš„æ¦‚å¿µï¼Œä»¥çº¿æ€§å›å½’å»æ‹Ÿåˆä¸€ä¸ª<strong>å¯¹æ•°å‡ ç‡</strong>(log-odds)ï¼Œå…¶æ¨¡å‹è¡¨è¾¾å¼ä¸ºï¼š</p>

\[\begin{aligned}
\hat{y}^{(i)}
 &amp;= \sigma(\theta_{0}x^{(i)}_{0}+\theta_{1}x^{(i)}_{1}+...+\theta_{n}x^{(i)}_{n}) \\
 &amp;= \sigma(x^{(i)}\theta^{T}) \\
\end{aligned}\]

<p>å…¶ä¸­ï¼Œ$\sigma(x)$ä¸ºï¼š</p>

\[\sigma(x)=\frac{1}{1+e^{-x}}\]

<p>å…¶å›¾åƒä¸ºï¼š</p>

<p><img src="img/Logistic-curve.svg" alt="" /></p>

<p>Logistic Regressionå®è´¨ä¸Šæ˜¯å°†çº¿æ€§å›å½’æ‰©å±•åˆ°äº†åˆ†ç±»ä»»åŠ¡ä¸Šï¼Œå¹¶æ”¯æŒæ¦‚ç‡è¾“å‡ºï¼Œå…¶è¡¨è¾¾å¼ä¸ºï¼š</p>

\[\hat{y}=\frac{1}{1+e^{-(x\theta^{T})}}\]

<p>ä¸ºäº†ç®€ä¾¿ï¼Œä¸Šå¼çœç•¥äº†æ ·æœ¬æ ‡å·$i$ï¼Œä¸‹åŒã€‚ç„¶åç»è¿‡ä¸€ç³»åˆ—å˜æ¢ï¼š</p>

\[\begin{aligned}
&amp; \hat{y}= \frac{1}{1+e^{-(x\theta^{T})}}= \frac{e^{x\theta^{T}}}{1+e^{x\theta^{T}}}\\
&amp; \frac{1}{\hat{y}} = 1+\frac{1}{e^{x\theta^{T}}} \\
&amp; \frac{1-\hat{y}}{\hat{y}} = \frac{1}{e^{x\theta^{T}}}
\end{aligned}\]

<p>å¾—ï¼š</p>

\[ln\frac{\hat{y}}{1-\hat{y}}=x^{(i)}\theta^{T}\]

<p><strong>æ³¨æ„</strong>ï¼Œç”±äº$\sigma(x)$å‡½æ•°çš„ä½œç”¨ï¼ŒLogistic Regressionçš„è¾“å‡ºå…¶å®æ˜¯ä¸€ä¸ªæ¦‚ç‡ï¼Œè¾“å…¥æ•°æ®ä¸ºæ­£æ ·æœ¬çš„æ¦‚ç‡ï¼Œå³ï¼š</p>

\[\begin{aligned}
\hat{y}&amp;=P(y=1|x;\theta) \\
1-\hat{y}&amp;=P(y=0|x;\theta) \\
\end{aligned}\]

<p>é‚£ä¹ˆï¼Œå‚æ•°$\theta$å…³äº$X$çš„ä¼¼ç„¶å‡½æ•°ä¸ºï¼š</p>

\[\begin{aligned}
L(\theta|X) &amp;= \prod_{i}P(y=0|x;\theta)\prod_{i}P(y=1|x;\theta) \\
			&amp;= \prod_{i}\hat{y}^{y}(1-\hat{y})^{1-y}
\end{aligned}\]

<p>å…¶å¯¹æ•°ä¼¼ç„¶å‡½æ•°ä¸ºï¼š</p>

\[\begin{aligned}
lnL(\theta|X) &amp;= \sum_{i}[yln(\hat{y})+(1-y)ln(1-\hat{y})] \\
&amp;= \sum_{i}[yln\frac{\hat{y}}{1-\hat{y}}+ln(1-\hat{y})] \\
&amp;= \sum_{i}[y*x\theta^{T}-ln(1+e^{x\theta^{T}})]
\end{aligned}\]

<p>æˆ‘ä»¬éœ€è¦æœ€å¤§åŒ–ä¼¼ç„¶å‡½æ•°ï¼Œé‚£ä¹ˆç­‰ä»·çš„æœ€å°åŒ–æŸå¤±å‡½æ•°ä¸ºï¼š</p>

\[Loss(\theta)=\sum_{i}[-y*x\theta^{T}+ln(1+e^{x\theta^{T}})]\]

<p>å¯¹äºlogistic regressionï¼ŒåŒæ ·å¯ä»¥ä½¿ç”¨æ¢¯åº¦ä¸‹é™æ³•æ¥ä¼˜åŒ–å‚æ•°$\theta$ã€‚æ³¨æ„sigmoidå‡½æ•°çš„å¯¼æ•°ï¼š</p>

\[\begin{aligned}
\frac{\partial{\sigma(x)}}{\partial{x}}&amp;=\frac{-1}{(1+e^{-x})^{2}}\cdot(-e^{-x}) \\
&amp;=\frac{1}{1+e^{-x}}\cdot\frac{e^{-x}+1-1}{1+e^{-x}} \\
&amp;=\frac{1}{1+e^{-x}}\cdot(1-\frac{1}{1+e^{-x}}) \\
&amp;=\sigma(x)\cdot(1-\sigma(x)) \\
\end{aligned}\]

<p>é‚£ä¹ˆåœ¨æ ‡é‡å½¢å¼ä¸‹ï¼Œæ˜“æ¨å¾—æŸå¤±å‡½æ•°å…³äºå‚æ•°$\thetaâ€‹$çš„æ¢¯åº¦ä¸ºï¼š</p>

\[\begin{aligned}
\frac{\partial{L}}{\partial{\theta}}&amp;=-\frac{y}{\hat{y}}{\cdot}\frac{\partial{\hat{y}}}{\partial{\theta}}+\frac{1-y}{1-\hat{y}}\cdot{\frac{\partial{\hat{y}}}{\partial\theta}} \\
&amp;=-\frac{y}{\hat{y}}{\cdot}\hat{y}(1-\hat{y}){\cdot{x}}+\frac{1-y}{1-\hat{y}}{\cdot}\hat{y}(1-\hat{y}){\cdot}x \\
&amp;=(\hat{y}-y)x
\end{aligned}\]

<p>æ³¨æ„åˆ°logistic regressionçš„æ¢¯åº¦å½¢å¼ä¸linear regressionæ˜¯ä¸€æ ·çš„ï¼Œå”¯ä¸€çš„åŒºåˆ«å°±åœ¨äº$\hat{y}$çš„ä¸åŒã€‚</p>

<h2 id="å†³ç­–è¾¹ç•Œ">å†³ç­–è¾¹ç•Œ</h2>

<p>ç”±äºLogistic Regressionçš„è¾“å‡ºæ˜¯ä¸€ä¸ª$p(\hat{y}=1)$çš„æ¦‚ç‡ï¼Œé‚£ä¹ˆå¯¹äºäºŒåˆ†ç±»ä»»åŠ¡ï¼Œæ¨¡å‹å¯¹æŸä¸€æ ·æœ¬åšå‡ºåˆ¤åˆ«çš„ä¾æ®å°±æ˜¯ä¸€ä¸ªæ¦‚ç‡é˜ˆå€¼ã€‚å‡å¦‚æ¦‚ç‡é˜ˆå€¼ä¸º0.5ï¼Œåˆ™å½“æ¨¡å‹è¾“å‡º$f(x)&gt;0.5$æ—¶åˆ¤ä¸ºæ­£æ ·æœ¬ï¼Œè€Œå½“æ¨¡å‹è¾“å‡º$f(x)&lt;0.5$æ—¶åˆ¤ä¸ºè´Ÿæ ·æœ¬ï¼Œæ­¤æ—¶æ¨¡å‹çš„å†³ç­–è¾¹ç•Œæ˜¯å•¥å‘¢ï¼Ÿ</p>

<p>å›é¡¾ä¸€ä¸‹$\sigma(x)$çš„å›¾åƒï¼Œ$\sigma(x)$æ°å¥½ç»è¿‡$(0, 0.5)$è¿™ä¸ªç‚¹ï¼Œå¹¶ä¸”æ˜¯å•å¢å‡½æ•°ï¼Œé‚£ä¹ˆå¯ä»¥çœ‹å‡ºï¼Œæ¨¡å‹çš„å†³ç­–è¾¹ç•Œä¸ºï¼š</p>

\[x^{(i)}\theta^{T}=\sigma(0.5)\]

<p>å½“ç„¶ï¼Œå†³ç­–è¾¹ç•Œä¼šæ ¹æ®è‡ªå®šä¹‰é˜ˆå€¼è€Œæ”¹å˜ï¼›é™¤æ­¤ä¹‹å¤–ï¼Œlogistic regressionä¹Ÿå¯ä»¥è®¾ç½®ä¸ºè¾“å‡ºè¿ç»­çš„æ¦‚ç‡å€¼ã€‚</p>

<p><a href="https://github.com/Daya-Jin/ML_for_learner/blob/master/linear_model/LogisticRegression.ipynb">å®ç°æŒ‡å¯¼</a></p>

<p><a href="https://github.com/Daya-Jin/ML_for_learner/blob/master/linear_model/LogisticRegression.py">å®Œæ•´ä»£ç </a></p>

<h1 id="åè®°">åè®°</h1>

<p>logistic regressionæœ¬è´¨ä¸Šè¿˜æ˜¯å±äºlinear modelçš„ä¸€ç§ï¼Œé‚£ä¹ˆlinear modelæ‰€å…·æœ‰çš„ä¼˜ç‚¹ï¼Œlogistic regressionä¹Ÿæ˜¯æœ‰çš„ï¼›å¯¹äºç¼ºç‚¹ä¹ŸåŒæ ·æˆç«‹ã€‚</p>

<p>ç”±äºlogistic regressionèƒŒåçš„æ¦‚ç‡æ€æƒ³ï¼Œå¦‚æœè®­ç»ƒæ ·æœ¬å­˜åœ¨æ ·æœ¬å¹³è¡¡æ€§é—®é¢˜ï¼Œé‚£ä¹ˆå°±ä¼šå¯¹è¯¥æ¨¡å‹çš„è¡¨ç°æœ‰å¾ˆå¤§çš„å½±å“ã€‚ç›´è§‚ä¸€ç‚¹æ¥è¯´ï¼Œlogistic regressionçš„å†³ç­–è¾¹ç•Œä¼šå—åˆ°æ ·æœ¬åˆ†å¸ƒå¯†åº¦çš„æ¨æŒ¤ï¼Œå…¶å†³ç­–è¾¹ç•Œä¼šæ¯”è¾ƒåè¿‘äºå°‘æ•°ç±»ã€‚</p>

<p>logistic regressionè¿˜æœ‰ä¸€ä¸ªè¢«è®¨è®ºçš„ç‚¹å°±æ˜¯å…³äºé«˜ç»´ç¨€ç–ç‰¹å¾çš„ã€‚</p>

<ol>
  <li>é¦–å…ˆï¼Œlogistic regressionä½œä¸ºä¸€ä¸ªçº¿æ€§æ¨¡å‹ï¼Œå°†ç‰¹å¾ä¹‹é—´åšç»„åˆå½¢æˆæ–°ç‰¹å¾æ˜¯å¢å¼ºçº¿æ€§æ¨¡å‹å¯¹éçº¿æ€§æ•°æ®æ‹Ÿåˆèƒ½åŠ›çš„å¿…è¦æ‰‹æ®µä¹‹ä¸€ï¼›</li>
  <li>å¦å¤–ï¼Œçº¿æ€§æ¨¡å‹è®¡ç®—ç®€å•ï¼Œåœ¨é«˜ç»´ç‰¹å¾ä¸‹çš„é€Ÿåº¦ä¹Ÿæ˜¯å¯ä»¥æ¥å—çš„ï¼›</li>
  <li>çº¿æ€§æ¨¡å‹çš„æ­£åˆ™åŒ–æ˜¯å¯¹å„ä¸ªç‰¹å¾çš„æƒé‡åšæƒ©ç½šï¼Œä¸ä¼šåœ¨æŸä¸€ç‰¹å¾ä¸Šäº§ç”Ÿè¿‡æ‹Ÿåˆï¼›</li>
  <li>æœ€åï¼Œå¯¹ç‰¹å¾çš„ç¦»æ•£åŒ–ï¼Œä¼šå¢å¼ºæ¨¡å‹å¯¹äºæ›´ç»†ç²’åº¦ç‰¹å¾çš„å­¦ä¹ èƒ½åŠ›ã€‚</li>
</ol>

<p>æ³¨æ„åˆ°ç”±å¯¹æ•°ä¼¼ç„¶å¾—åˆ°çš„logistic regressionæŸå¤±å‡½æ•°å¯ä»¥å†™æˆï¼š</p>

\[\begin{aligned}
    Loss&amp;=-\sum_{i}[y\ln(\hat{y})+(1-y)\ln(1-\hat{y})] \\
    &amp;=\sum_{i}[y\ln\frac{1}{\hat{y}}+(1-y)\ln\frac{1}{1-\hat{y}}] \\
\end{aligned}\]

<p>åœ¨ä¿¡æ¯è®ºä¸­ï¼Œ$H(P,Q)=\sum\limits_{i}P(i)\log\frac{1}{Q(i)}$ç§°ä¸ºäº¤å‰ç†µï¼Œå…¶ä¸­$P$ä¸ºçœŸå®åˆ†å¸ƒï¼Œ$Q$ä¸ºéçœŸå®åˆ†å¸ƒï¼Œäº¤å‰ç†µå¯ç”¨äºè¡¡é‡ä¸¤ä¸ªåˆ†å¸ƒä¹‹é—´çš„å·®å¼‚æ€§ã€‚ä¸éš¾å‘ç°ï¼Œlogistic regressionçš„æŸå¤±å‡½æ•°å®é™…ä¸Šå°±æ˜¯ä¸€ä¸ªäº¤å‰ç†µã€‚</p>
:ET