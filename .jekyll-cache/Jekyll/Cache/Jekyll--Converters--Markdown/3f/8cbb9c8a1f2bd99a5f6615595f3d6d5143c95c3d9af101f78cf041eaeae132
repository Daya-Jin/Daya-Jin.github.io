I"a<ul id="markdown-toc">
  <li><a href="#概述" id="markdown-toc-概述">概述</a>    <ul>
      <li><a href="#information-theory" id="markdown-toc-information-theory">Information Theory</a>        <ul>
          <li><a href="#information" id="markdown-toc-information">Information</a></li>
          <li><a href="#entropy" id="markdown-toc-entropy">Entropy</a></li>
          <li><a href="#cross-entropy" id="markdown-toc-cross-entropy">Cross entropy</a></li>
          <li><a href="#kl-divergence" id="markdown-toc-kl-divergence">KL Divergence</a></li>
          <li><a href="#maximum-likelihood-estimation" id="markdown-toc-maximum-likelihood-estimation">Maximum Likelihood Estimation</a></li>
          <li><a href="#maximum-a-posteriori" id="markdown-toc-maximum-a-posteriori">Maximum A Posteriori</a></li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h1 id="概述">概述</h1>

<h2 id="information-theory">Information Theory</h2>

<h3 id="information">Information</h3>

<p>一个事件的信息量被定义为：</p>

\[I(x)=-\log{P(x)}\]

<p>易得信息量的大小和事件发生的概率成反比，当某事件绝对发生($P(x)$=1)时，该事件不含信息量。</p>

<h3 id="entropy">Entropy</h3>

<p>熵可以衡量信息量的大小，定义为：</p>

\[\begin{aligned}
    H(x)&amp;=-\mathbb{E}_{x\sim{P}}[\log{P(x)}] \\
    &amp;=-\sum{P(x)}\log{P(x)} \\
\end{aligned}\]

<p>熵越大，说明事件越具有随机性，那么所包含的信息量就越大。特别地，当$\log$函数以$2$为底时，信息熵指示了编码事件所有信息所需要的编码长度。如<em>掷硬币</em>这一事件的信息熵为：</p>

\[\begin{aligned}
    H(coin)&amp;=-P(head)\log{P(head)}-P(tail)\log{P(tail)} \\
    &amp;=-\frac{1}{2}\log{\frac{1}{2}}-\frac{1}{2}\log{\frac{1}{2}} \\
    &amp;=1
\end{aligned}\]

<p>由此得编码<em>掷硬币</em>这一事件只需要$1$位。</p>

<h3 id="cross-entropy">Cross entropy</h3>

<p>交叉熵可以用于衡量两个分布之间的差异性，定义为：</p>

\[\begin{aligned}
    H(P,Q)&amp;=-\mathbb{E}_{x\sim{P}}[\log{Q(x)}] \\
    &amp;=-\sum{P(x)}\log{Q(x)} \\
\end{aligned}\]

<p>信息熵表示使用自身分布来编码信息所需要的位数，而交叉熵表示用一个错误分布$Q$来编码真实分布$P$所需要的平均位数。</p>

<h3 id="kl-divergence">KL Divergence</h3>

<p>KL散度也称相对熵，是用于两个分布差异的方法之，其定义为：</p>

\[\begin{aligned}
    KL(P\vert\vert{Q})&amp;=\mathbb{E}_{x\sim{P}}\log\frac{P(x)}{Q(x)} \\
    &amp;=\sum{P(x)[\log{P(x)-\log{Q(x)}}]}
\end{aligned}\]

<p>注意KL散度具有不对称性。</p>

<h3 id="maximum-likelihood-estimation">Maximum Likelihood Estimation</h3>

<p>如果已观测到事件$X$的一系列发生概率，求使得这一系列概率出现可能性最大的参数$\theta$，使用最大似然估计：</p>

\[\hat{\theta}=\arg\max\limits_{\theta}\prod{P(x_{i}\vert\theta)}\]

<p>其中$p(x_{i}\vert\theta)$为事件$x_{i}$在参数$\theta$下的发生概率。特别地，如果某条件概率为：</p>

\[\begin{aligned}
    p(y\vert{x};\theta)&amp;\sim{\mathcal{N}(x\theta,\sigma^{2})} \\
    &amp;=\frac{1}{\sigma\sqrt{2\pi}}\exp(\frac{-(y-x\theta)^{2}}{2\sigma^{2}}) \\
\end{aligned}\]

<p>参数$\theta$在已有观测样本${(x_{i},y_{i})}$下的最大似然为：</p>

\[\begin{aligned}
    \hat{\theta}&amp;=\arg\max\limits_{\theta}\prod{P(y_{i}|x_{i};\theta)} \\
    &amp;=\arg\max\limits_{\theta}\sum{\log\frac{1}{\sigma\sqrt{2\pi}}\exp(\frac{-(y_{i}-x_{i}\theta)^{2}}{2\sigma^{2}})} \\
    &amp;=\arg\max\limits_{\theta}\sum{[\log\frac{1}{\sigma\sqrt{2\pi}}-\frac{(y_{i}-x_{i}\theta)^{2}}{2\sigma^{2}}]} \\
    &amp;=\arg\min\limits_{\theta}(y_{i}-x_{i}\theta)^{2}
\end{aligned}\]

<h3 id="maximum-a-posteriori">Maximum A Posteriori</h3>

<p>在最大似然估计中，对于参数$\theta$没有做任何假设，意味着$\theta$可以服从任何分布，只要能使得观测事件发生的概率最大即可。假如在某些情况下，参数$\theta$也是服从某一分布的，那么最大似然估计就不再适用于参数估计了，而应该使用最大后验概率：</p>

\[\begin{aligned}
    \hat{\theta}&amp;=\arg\max\limits_{\theta}\prod{P(\theta|x_{i})} \\
    &amp;=\arg\max\limits_{\theta}\prod{\frac{P(x_{i}\vert\theta)P(\theta)}{P(x_{i})}}
\end{aligned}\]

<p>看可以看出MAP引入了参数$\theta$的先验分布。</p>
:ET